from __future__ import annotations
from typing import Iterable, Dict, Any, Optional
from datetime import datetime, timezone, timedelta
import re

import psycopg
from psycopg_pool import ConnectionPool
from psycopg.types.json import Json
from psycopg.rows import dict_row

from app.core.config import settings

pool: ConnectionPool | None = None

DDL_CREATE = """
CREATE TABLE IF NOT EXISTS trending_keywords (
  id                  BIGSERIAL PRIMARY KEY,
  collected_at        TIMESTAMPTZ NOT NULL,
  geo                 TEXT,
  hl                  TEXT,
  hours               INT,
  title               TEXT NOT NULL,        -- = query(í˜¸í™˜ì„±ì„ ìœ„í•´ title ì»¬ëŸ¼ ìœ ì§€)
  link                TEXT,                 -- ê¸°ì¡´ í˜¸í™˜ìš© (ì—†ìœ¼ë©´ NULL)
  categories          TEXT,                 -- ì¹´í…Œê³ ë¦¬ nameë“¤ì„ '|'ë¡œ í•©ì¹œ ë¬¸ìì—´
  search_volume       INT,
  increase_percentage INT,
  active              BOOLEAN,
  start_time          TIMESTAMPTZ,          -- start_timestamp(ì´ˆ) -> UTC ë³€í™˜
  trends_link         TEXT,                 -- serpapi_google_trends_link
  news_page_token     TEXT,                 -- news_page_token
  news_link           TEXT,                 -- serpapi_news_link
  raw_json            JSONB
);

-- ì¸ë±ìŠ¤
CREATE INDEX IF NOT EXISTS idx_trending_keywords_collected_at ON trending_keywords (collected_at DESC);
CREATE INDEX IF NOT EXISTS idx_trending_keywords_title        ON trending_keywords (title);

-- ë„¤ì´ë²„ ë­í‚¹ë‰´ìŠ¤ í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS naver_ranking_news (
  id           BIGSERIAL PRIMARY KEY,
  collected_at TIMESTAMPTZ NOT NULL,
  press        TEXT        NOT NULL,   -- ì–¸ë¡ ì‚¬ ì´ë¦„
  category     TEXT,                   -- ì„¹ì…˜(ì •ì¹˜/ê²½ì œ/ì‚¬íšŒ ë“±), ì—†ìœ¼ë©´ NULL
  rank         INT         NOT NULL,   -- ì–¸ë¡ ì‚¬ë³„ ë­í‚¹ ìˆœìœ„
  title        TEXT        NOT NULL,   -- ê¸°ì‚¬ ì œëª©
  link         TEXT        NOT NULL,   -- ê¸°ì‚¬ ë§í¬
  raw_json     JSONB                   -- ì›ë³¸ ì „ì²´ JSON
);

CREATE INDEX IF NOT EXISTS idx_naver_ranking_collected_at ON naver_ranking_news (collected_at DESC);
CREATE INDEX IF NOT EXISTS idx_naver_ranking_press        ON naver_ranking_news (press);
CREATE INDEX IF NOT EXISTS idx_naver_ranking_title        ON naver_ranking_news (title);
"""

def init_pool():
    """Initialize the global connection pool and ensure DDL exists."""
    global pool
    if pool is None:
        if not settings.DATABASE_URL:
            raise RuntimeError("DATABASE_URL is empty")
        pool = ConnectionPool(settings.DATABASE_URL, min_size=1, max_size=4, open=True)
        with pool.connection() as conn:
            conn.execute(DDL_CREATE)

def close_pool():
    global pool
    if pool:
        pool.close()
        pool = None

def _epoch_to_ts(epoch: Optional[int]) -> Optional[datetime]:
    if epoch is None:
        return None
    try:
        return datetime.fromtimestamp(int(epoch), tz=timezone.utc)
    except Exception:
        return None

def _categories_pipe(item: Dict[str, Any]) -> Optional[str]:
    cats = item.get("categories")
    if isinstance(cats, list) and cats:
        names = [c.get("name") for c in cats if isinstance(c, dict) and c.get("name")]
        if names:
            return "|".join(names)
    return None

def _normalize_item_for_insert(it: Dict[str, Any]) -> Dict[str, Any]:
    # SerpAPI data.trending_searches[] í‘œì¤€ í‚¤ë“¤ ë§¤í•‘
    query = it.get("query") or it.get("title") or it.get("name")
    link = it.get("link") or it.get("explore_link")  # í˜¸í™˜ìš©(ì—†ì–´ë„ ë¬´ë°©)
    start_ts = _epoch_to_ts(it.get("start_timestamp"))
    categories = _categories_pipe(it)

    return {
        "title": query,  # DB ì»¬ëŸ¼ì€ title ì´ë¦„ ìœ ì§€
        "link": link,
        "categories": categories,
        "search_volume": it.get("search_volume"),
        "increase_percentage": it.get("increase_percentage"),
        "active": it.get("active"),
        "start_time": start_ts,
        "trends_link": it.get("serpapi_google_trends_link"),
        "news_page_token": it.get("news_page_token"),
        "news_link": it.get("serpapi_news_link"),
        "raw": it,
    }

def save_keywords(geo: str, hl: str, hours: int, items: Iterable[Dict[str, Any]]) -> int:
    """Bulk-insert trending keywords collected at the same time."""
    if pool is None:
        raise RuntimeError("Pool not initialized")

    now = datetime.now(timezone.utc)
    rows = []
    for it in items:
        n = _normalize_item_for_insert(it)
        if not n.get("title"):
            continue  # ì œëª©(=query) ì—†ìœ¼ë©´ ìŠ¤í‚µ
        rows.append((
            now, geo, hl, hours,
            n["title"], n["link"],
            n["categories"], n["search_volume"], n["increase_percentage"], n["active"], n["start_time"],
            n["trends_link"], n["news_page_token"], n["news_link"],
            Json(n["raw"]),
        ))

    if not rows:
        return 0

    sql = """
    INSERT INTO trending_keywords (
      collected_at, geo, hl, hours,
      title, link,
      categories, search_volume, increase_percentage, active, start_time,
      trends_link, news_page_token, news_link,
      raw_json
    )
    VALUES (
      %s,%s,%s,%s,
      %s,%s,
      %s,%s,%s,%s,%s,
      %s,%s,%s,
      %s
    );
    """
    with pool.connection() as conn:
        with conn.cursor() as cur:
            cur.executemany(sql, rows)
        conn.commit()
    return len(rows)

# ---------------------------
# ì‹ ê·œ: ìƒìœ„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¡°íšŒ
# ---------------------------

def _category_regex(category: str) -> str:
    """
    categories ì»¬ëŸ¼ì´ 'A|B|C' í˜•íƒœì¼ ë•Œ ì •í™•í•œ í† í° ë§¤ì¹­ì„ ìœ„í•œ ì •ê·œì‹ íŒ¨í„´ì„ ìƒì„±.
    ëŒ€ì†Œë¬¸ì ë¬´ì‹œ(~* ì‚¬ìš©).
    """
    safe = re.escape(category.strip())
    # ê²½ê³„: ì‹œì‘/ë ë˜ëŠ” íŒŒì´í”„(|)
    return rf"(^|\|){safe}($|\|)"

def get_top_trending_keyword(category: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """
    ìµœê·¼ 4ì‹œê°„ ë‚´(collected_at ê¸°ì¤€), search_volume >= 500 ì¡°ê±´ì—ì„œ
    (ì„ íƒ) ì¹´í…Œê³ ë¦¬ ì •í™• ë§¤ì¹­ìœ¼ë¡œ í•„í„°í•˜ì—¬ ê°€ì¥ ë†’ì€ ê²€ìƒ‰ì–´ í•˜ë‚˜ë¥¼ ë°˜í™˜.
    - category ê°€ ì£¼ì–´ì¡ŒëŠ”ë° í•´ë‹¹ ë²”ìœ„ì— ë°ì´í„° ì—†ìœ¼ë©´ None.
    - category ê°€ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ì„ íƒ.
    ë°˜í™˜ê°’: dict(ì»¬ëŸ¼ ì „ë¶€ í¬í•¨) ë˜ëŠ” None
    """
    if pool is None:
        raise RuntimeError("Pool not initialized")

    cutoff = datetime.now(timezone.utc) - timedelta(hours=4)

    sql_base = """
        SELECT
            id, collected_at, geo, hl, hours,
            title, link, categories, search_volume, increase_percentage,
            active, start_time, trends_link, news_page_token, news_link, raw_json
        FROM trending_keywords
        WHERE collected_at >= %s
          AND (search_volume IS NOT NULL AND search_volume >= 500)
    """

    args = [cutoff]

    if category and category.strip():
        # ì •í™• ë§¤ì¹­ ì •ê·œì‹ (~* : case-insensitive)
        pattern = _category_regex(category)
        sql = sql_base + " AND categories IS NOT NULL AND categories ~* %s "
        args.append(pattern)
    else:
        sql = sql_base

    # ìš°ì„ ìˆœìœ„ ì •ë ¬
    sql += """
        ORDER BY
            search_volume DESC NULLS LAST,
            increase_percentage DESC NULLS LAST,
            collected_at DESC
        LIMIT 1
    """

    with pool.connection() as conn:
        with conn.cursor(row_factory=dict_row) as cur:
            cur.execute(sql, args)
            row = cur.fetchone()

    return dict(row) if row else None

# ---------------------------
# ì‹ ê·œ: ë„¤ì´ë²„ ë­í‚¹ë‰´ìŠ¤ ì €ì¥
# ---------------------------

def save_naver_ranking_news(items: Iterable[Dict[str, Any]]) -> int:
    """
    ë„¤ì´ë²„ ë­í‚¹ë‰´ìŠ¤ ëª©ë¡ì„ naver_ranking_news í…Œì´ë¸”ì— ì €ì¥.
    - ì œëª©(title) ê¸°ì¤€ìœ¼ë¡œ UNIQUE.
    - ì´ë¯¸ ê°™ì€ ì œëª©ì´ ìˆìœ¼ë©´ 500 ì—ëŸ¬ ëŒ€ì‹  ê·¸ëƒ¥ ë¬´ì‹œ(ì‚½ì… ì•ˆ í•¨).
    - ì €ì¥ ì‹œì ì— ê¸°ì¤€ìœ¼ë¡œ 3ì¼ ì´ì „ ë°ì´í„°ëŠ” ë¨¼ì € ì‚­ì œ.
    """
    if pool is None:
        raise RuntimeError("Pool not initialized")

    now = datetime.now(timezone.utc)
    rows = []

    for it in items:
        press = it.get("press")
        title = it.get("title")
        link = it.get("link")
        rank = it.get("rank")
        category = it.get("category")

        if not press or not title or not link or rank is None:
            continue

        try:
            rank_int = int(rank)
        except Exception:
            continue

        rows.append(
            (now, press, category, rank_int, title, link, Json(it))
        )

    if not rows:
        return 0

    sql = """
    INSERT INTO naver_ranking_news (
      collected_at,
      press,
      category,
      rank,
      title,
      link,
      raw_json
    )
    VALUES (%s,%s,%s,%s,%s,%s,%s)
    ON CONFLICT (title) DO NOTHING;
    """

    with pool.connection() as conn:
        with conn.cursor() as cur:
            # ğŸ”¹ ë¨¼ì € 3ì¼ ì´ì „ ë°ì´í„° ì‚­ì œ
            cur.execute(
                """
                DELETE FROM naver_ranking_news
                 WHERE collected_at < NOW() - INTERVAL '3 days'
                """
            )
            # ğŸ”¹ ê·¸ ë‹¤ìŒ ìƒˆ ë°ì´í„° ì‚½ì…
            cur.executemany(sql, rows)
        conn.commit()

    return len(rows)

def get_top_news(category: str | None = None) -> Optional[Dict[str, Any]]:
    """
    24ì‹œê°„ ë‚´ ìµœì‹ ë‰´ìŠ¤ ì¤‘ ëœë¤ 1ê°œ ì¶”ì¶œ.
    category ë‹¤ì¤‘ ì…ë ¥ ê°€ëŠ¥: "ì •ì¹˜|ê²½ì œ|ì‚¬íšŒ"
    """
    if pool is None:
        raise RuntimeError("Pool not initialized")

    # ê¸°ë³¸ SQL
    base_sql = """
        SELECT id, press, rank, title
          FROM naver_ranking_news
         WHERE collected_at >= NOW() - INTERVAL '24 hours'
    """

    params = []

    # ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬
    if category:
        cats = [c.strip() for c in category.split("|") if c.strip()]
        if cats:
            placeholders = ",".join(["%s"] * len(cats))
            base_sql += f" AND category IN ({placeholders})"
            params.extend(cats)

    # ëœë¤ ìˆœì„œ
    base_sql += """
         ORDER BY RANDOM()
         LIMIT 1;
    """

    with pool.connection() as conn:
        with conn.cursor(row_factory=dict_row) as cur:
            cur.execute(base_sql, params)
            row = cur.fetchone()

    return dict(row) if row else None